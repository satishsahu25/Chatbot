{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMFX2pSzAsyqtEe8bmeVFg4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"id":"SEj0wPz2tODG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677387531291,"user_tz":-330,"elapsed":6144,"user":{"displayName":"Satish Sahu","userId":"00112273144191307121"}},"outputId":"8c3e231f-3872-4ebf-c15f-3062fad25744"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["# For NLP\n","import nltk\n","# natural language toolkit for human response understanding\n","nltk.download('punkt') #for tokenization of sentences into words\n","from nltk.stem.lancaster import LancasterStemmer  #used in reducing the different form of words into single base word\n","#like cook, cooked, cooking into base/stem word: cook\n","stemmer=LancasterStemmer()\n"]},{"cell_type":"code","source":["pip install tflearn"],"metadata":{"id":"UW-Wva74t5_r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677387538452,"user_tz":-330,"elapsed":7167,"user":{"displayName":"Satish Sahu","userId":"00112273144191307121"}},"outputId":"2c7dcf8d-9abb-4ec0-e718-e04b99f894fb"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tflearn\n","  Downloading tflearn-0.5.0.tar.gz (107 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.3/107.3 KB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tflearn) (1.22.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from tflearn) (1.15.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from tflearn) (7.1.2)\n","Building wheels for collected packages: tflearn\n","  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127299 sha256=b07bf607e85f7c06379aeb43e93bd4ab9aee10e09cd123aa34e8bc3cb5baf7bf\n","  Stored in directory: /root/.cache/pip/wheels/65/9b/15/cb1e6b279c14ed897530d15cfd7da8e3df8a947e593f5cfe59\n","Successfully built tflearn\n","Installing collected packages: tflearn\n","Successfully installed tflearn-0.5.0\n"]}]},{"cell_type":"code","source":["# Tensorflow processing\n","import tensorflow as tf\n","import numpy as np  \n","import tflearn #for different layers in deep neural netwrok\n","import random #to generate the numbers\n","import json #to read the intent json file and train the model"],"metadata":{"id":"6QRj_LVVt51N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677387541308,"user_tz":-330,"elapsed":2860,"user":{"displayName":"Satish Sahu","userId":"00112273144191307121"}},"outputId":"988b3f77-d76f-4e94-8c2c-b9f13f27a266"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n"]}]},{"cell_type":"code","source":["from google.colab import files\n","files.upload()"],"metadata":{"id":"FHMdyKtkt6Cj","colab":{"base_uri":"https://localhost:8080/","height":351},"executionInfo":{"status":"ok","timestamp":1677387643937,"user_tz":-330,"elapsed":102635,"user":{"displayName":"Satish Sahu","userId":"00112273144191307121"}},"outputId":"f45106a3-1fd0-443a-906e-c73c9bf3ffe8"},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-4b35737b-b972-4465-9940-f66e23f8170f\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-4b35737b-b972-4465-9940-f66e23f8170f\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving intents.json to intents.json\n"]},{"output_type":"execute_result","data":{"text/plain":["{'intents.json': b'{\"intents\": [\\r\\n        {\"tag\": \"greeting\",\\r\\n         \"patterns\": [\"Hi\", \"How are you\", \"Is anyone there?\", \"Hello\", \"Good day\"],\\r\\n         \"responses\": [\"Hello, thanks for visiting\", \"Good to see you again\", \"Hi there, how can I help?\"],\\r\\n         \"context_set\": \"\"\\r\\n        },\\r\\n        {\"tag\": \"goodbye\",\\r\\n         \"patterns\": [\"Bye\", \"See you later\", \"Goodbye\"],\\r\\n         \"responses\": [\"See you later, thanks for visiting\", \"Have a nice day\", \"Bye! Come back again soon.\"]\\r\\n        },\\r\\n        {\"tag\": \"thanks\",\\r\\n         \"patterns\": [\"Thanks\", \"Thank you\", \"That\\'s helpful\"],\\r\\n         \"responses\": [\"Happy to help!\", \"Any time!\", \"My pleasure\"]\\r\\n        },\\r\\n        {\"tag\": \"hours\",\\r\\n         \"patterns\": [\"What hours are you open?\", \"What are your hours?\", \"When are you open?\" ],\\r\\n         \"responses\": [\"We\\'re open every day 9am-9pm\", \"Our hours are 9am-9pm every day\"]\\r\\n        },\\r\\n        {\"tag\": \"location\",\\r\\n         \"patterns\": [\"What is your location?\", \"Where are you located?\", \"What is your address?\", \"Where is your restaurant situated?\" ],\\r\\n         \"responses\": [\"We are on the intersection of London Alley and Bridge Avenue.\", \"We are situated at the intersection of London Alley and Bridge Avenue\", \"Our Address is: 1000 Bridge Avenue, London EC3N 4AJ, UK\"]\\r\\n        },\\r\\n        {\"tag\": \"payments\",\\r\\n         \"patterns\": [\"Do you take credit cards?\", \"Do you accept Mastercard?\", \"Are you cash only?\" ],\\r\\n         \"responses\": [\"We accept VISA, Mastercard and AMEX\", \"We accept most major credit cards\"]\\r\\n        },\\r\\n        {\"tag\": \"todaysmenu\",\\r\\n         \"patterns\": [\"What is your menu for today?\", \"What are you serving today?\", \"What is today\\'s special?\"],\\r\\n         \"responses\": [\"Today\\'s special is Chicken Tikka\", \"Our speciality for today is Chicken Tikka\"]\\r\\n        },\\r\\n        {\"tag\": \"deliveryoption\",\\r\\n         \"patterns\": [\"Do you provide home delivery?\", \"Do you deliver the food?\", \"What are the home delivery options?\" ],\\r\\n         \"responses\": [\"Yes, we provide home delivery through UBER Eats and Zomato?\", \"We have home delivery options through UBER Eats and Zomato\"],\\r\\n         \"context_set\": \"food\"\\r\\n        },\\r\\n        {\"tag\": \"menu\",\\r\\n         \"patterns\": [\"What is your Menu?\", \"What are the main course options?\", \"Can you tell me the most delicious dish from the menu?\", \"What is the today\\'s special?\"],\\r\\n         \"responses\": [\"You can visit www.mymenu.com for menu options\", \"You can check out the food menu at www.mymenu.com\", \"You can check various delicacies given in the food menu at www.mymenu.com\"],\\r\\n         \"context_filter\": \"food\"\\r\\n        }\\r\\n   ]\\r\\n}'}"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["# importing the chatbot intents\n","with open('intents.json') as json_data:\n","  intents=json.load(json_data)"],"metadata":{"id":"RRsm8ettt6FG","executionInfo":{"status":"ok","timestamp":1677387643937,"user_tz":-330,"elapsed":15,"user":{"displayName":"Satish Sahu","userId":"00112273144191307121"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["intents"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H95GUNgxt6H5","executionInfo":{"status":"ok","timestamp":1677387643937,"user_tz":-330,"elapsed":12,"user":{"displayName":"Satish Sahu","userId":"00112273144191307121"}},"outputId":"ad710b4a-a8ab-4094-a8a4-f309caae6453"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'intents': [{'tag': 'greeting',\n","   'patterns': ['Hi', 'How are you', 'Is anyone there?', 'Hello', 'Good day'],\n","   'responses': ['Hello, thanks for visiting',\n","    'Good to see you again',\n","    'Hi there, how can I help?'],\n","   'context_set': ''},\n","  {'tag': 'goodbye',\n","   'patterns': ['Bye', 'See you later', 'Goodbye'],\n","   'responses': ['See you later, thanks for visiting',\n","    'Have a nice day',\n","    'Bye! Come back again soon.']},\n","  {'tag': 'thanks',\n","   'patterns': ['Thanks', 'Thank you', \"That's helpful\"],\n","   'responses': ['Happy to help!', 'Any time!', 'My pleasure']},\n","  {'tag': 'hours',\n","   'patterns': ['What hours are you open?',\n","    'What are your hours?',\n","    'When are you open?'],\n","   'responses': [\"We're open every day 9am-9pm\",\n","    'Our hours are 9am-9pm every day']},\n","  {'tag': 'location',\n","   'patterns': ['What is your location?',\n","    'Where are you located?',\n","    'What is your address?',\n","    'Where is your restaurant situated?'],\n","   'responses': ['We are on the intersection of London Alley and Bridge Avenue.',\n","    'We are situated at the intersection of London Alley and Bridge Avenue',\n","    'Our Address is: 1000 Bridge Avenue, London EC3N 4AJ, UK']},\n","  {'tag': 'payments',\n","   'patterns': ['Do you take credit cards?',\n","    'Do you accept Mastercard?',\n","    'Are you cash only?'],\n","   'responses': ['We accept VISA, Mastercard and AMEX',\n","    'We accept most major credit cards']},\n","  {'tag': 'todaysmenu',\n","   'patterns': ['What is your menu for today?',\n","    'What are you serving today?',\n","    \"What is today's special?\"],\n","   'responses': [\"Today's special is Chicken Tikka\",\n","    'Our speciality for today is Chicken Tikka']},\n","  {'tag': 'deliveryoption',\n","   'patterns': ['Do you provide home delivery?',\n","    'Do you deliver the food?',\n","    'What are the home delivery options?'],\n","   'responses': ['Yes, we provide home delivery through UBER Eats and Zomato?',\n","    'We have home delivery options through UBER Eats and Zomato'],\n","   'context_set': 'food'},\n","  {'tag': 'menu',\n","   'patterns': ['What is your Menu?',\n","    'What are the main course options?',\n","    'Can you tell me the most delicious dish from the menu?',\n","    \"What is the today's special?\"],\n","   'responses': ['You can visit www.mymenu.com for menu options',\n","    'You can check out the food menu at www.mymenu.com',\n","    'You can check various delicacies given in the food menu at www.mymenu.com'],\n","   'context_filter': 'food'}]}"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["words=[] #storing individual words\n","classes=[] #storing the tags/classes of intent\n","documents=[] \n","ignore=['?','!'] #punctuation/speacial marks to be excluded from words\n","\n","for intent in intents['intents']:\n","  for pattern in intent['patterns']:\n","    # tokenization of each word in every sentence/pattern of patterns\n","    w=nltk.word_tokenize(pattern)\n","    words.extend(w) #adding individual words in empty list \n","    documents.append((w,intent['tag'])) #adding words in documents for each tags\n","    # adding each tags in classes list\n","    if intent['tag'] not in classes:\n","      classes.append(intent['tag'])"],"metadata":{"id":"y0GKK8tzt6Kn","executionInfo":{"status":"ok","timestamp":1677387643938,"user_tz":-330,"elapsed":11,"user":{"displayName":"Satish Sahu","userId":"00112273144191307121"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# converting into lowercase and removing duplicates and stemming of words\n","words=[stemmer.stem(w.lower()) for w in words if w not in ignore] #converts in lowercase+stemming+ignoring the ignore words\n","words=sorted(list(set(words))) #removing the duplicate words by set operation & storing alphabetically"],"metadata":{"id":"Fea4-C_et6Mn","executionInfo":{"status":"ok","timestamp":1677387643938,"user_tz":-330,"elapsed":10,"user":{"displayName":"Satish Sahu","userId":"00112273144191307121"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# removing duplicate classes\n","classes=sorted(list(set(classes)))\n","\n","print(len(documents),\"documents\")\n","print(len(classes),\"classses\",classes)\n","print(len(words),\"unique stemmed words\",words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sA3qIVgCt6PU","executionInfo":{"status":"ok","timestamp":1677387643938,"user_tz":-330,"elapsed":10,"user":{"displayName":"Satish Sahu","userId":"00112273144191307121"}},"outputId":"08ff534e-3c48-4c84-ab18-aa0619bccc7e"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["31 documents\n","9 classses ['deliveryoption', 'goodbye', 'greeting', 'hours', 'location', 'menu', 'payments', 'thanks', 'todaysmenu']\n","57 unique stemmed words [\"'s\", 'acceiv', 'address', 'anyon', 'ar', 'bye', 'can', 'card', 'cash', 'cours', 'credit', 'day', 'del', 'delicy', 'delivery', 'dish', 'do', 'food', 'for', 'from', 'good', 'goodby', 'hello', 'help', 'hi', 'hom', 'hour', 'how', 'is', 'lat', 'loc', 'main', 'mastercard', 'me', 'menu', 'most', 'on', 'op', 'opt', 'provid', 'resta', 'see', 'serv', 'situ', 'spec', 'tak', 'tel', 'thank', 'that', 'the', 'ther', 'today', 'what', 'when', 'wher', 'yo', 'you']\n"]}]},{"cell_type":"code","source":["# creating the training data set\n","training=[]\n","output=[]\n","# create empty array for output\n","output_empty=[0]*len(classes)\n","\n","# creating training set, bag of words for each sentence\n","for doc in documents:\n","  # for bag of words array\n","  bag=[]\n","  # list of tokenized words\n","  pattern_words=doc[0]\n","  # stemming each word\n","  pattern_words=[stemmer.stem(word.lower()) for word in pattern_words]\n","  # for current tag append 1 and for rest append 0 below \n","  # by matching the patternword with words array\n","  for w in words:\n","    bag.append(1) if w in pattern_words else bag.append(0)\n","  \n","  # for current tag output is 1 and 0 for rest other tags\n","  output_row=list(output_empty)\n","  output_row[classes.index(doc[1])]=1\n","\n","  training.append([bag,output_row])\n","\n","# shuffling features and turning into np.array\n","random.shuffle(training)\n","training=np.array(training)\n","\n","# creating training lists\n","# features(intent patterns)\n","trainx=list(training[:,0])\n","# labels(associated tags or labelled class)\n","trainy=list(training[:,1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1HhWshb0t6SE","executionInfo":{"status":"ok","timestamp":1677387643938,"user_tz":-330,"elapsed":7,"user":{"displayName":"Satish Sahu","userId":"00112273144191307121"}},"outputId":"e76e230b-25c7-4d60-f64d-3e6a8a321d87"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-12-ed40cee26c2a>:28: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  training=np.array(training)\n"]}]},{"cell_type":"code","source":["# restting underlying graph data\n","tf.compat.v1.reset_default_graph()\n","\n","# building the neural network\n","# input layer\n","net=tflearn.input_data(shape=[None,len(trainx[0])])\n","# 2 hidden layers\n","net=tflearn.fully_connected(net,10)\n","net=tflearn.fully_connected(net,10)\n","# ouptut layer having trainy[0] number of fetaures\n","net=tflearn.fully_connected(net,len(trainy[0]),activation=\"softmax\")\n","# applying the regression on the input\n","net=tflearn.regression(net)\n","\n","\n","# dnn model\n","model=tflearn.DNN(net,tensorboard_dir='tflearn_logs')\n","\n","# start training\n","# more number of epochs more accuarcy, show_metric for showing the performancereprot\n","model.fit(trainx,trainy,n_epoch=1000,batch_size=8,show_metric=True)\n","# saving the model so that no need to train again and again\n","# but if new intents are provided then we have to retrain the model\n","model.save('model.tflearn')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LUnrsyO-t6Ut","executionInfo":{"status":"ok","timestamp":1677387802593,"user_tz":-330,"elapsed":158661,"user":{"displayName":"Satish Sahu","userId":"00112273144191307121"}},"outputId":"7c3e62e0-57f0-4726-b7a2-792990a427c7"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Step: 3999  | total loss: \u001b[1m\u001b[32m0.02105\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 1000 | loss: 0.02105 - acc: 1.0000 -- iter: 24/31\n","Training Step: 4000  | total loss: \u001b[1m\u001b[32m0.02054\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 1000 | loss: 0.02054 - acc: 1.0000 -- iter: 31/31\n","--\n"]}]},{"cell_type":"code","source":["import pickle\n","#saving the model and intermediate data structrue\n","pickle.dump({'words':words,'classes':classes,'trainx':trainx,'trainy':trainy},open(\"training_data\",\"wb\"))"],"metadata":{"id":"Gh_iB5Gct6Xe","executionInfo":{"status":"ok","timestamp":1677387802594,"user_tz":-330,"elapsed":20,"user":{"displayName":"Satish Sahu","userId":"00112273144191307121"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["pickle.dump(model,open(\"trainedmodel.pkl\",'wb'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":165},"id":"rf__2RYtHjVf","executionInfo":{"status":"error","timestamp":1677388065653,"user_tz":-330,"elapsed":429,"user":{"displayName":"Satish Sahu","userId":"00112273144191307121"}},"outputId":"258e2067-fa95-447d-a347-2c6e37f403c8"},"execution_count":23,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-c280fbfec23c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trainedmodel.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: cannot pickle '_thread.RLock' object"]}]},{"cell_type":"code","source":["# restoring the all data structrues\n","data=pickle.load(open('training_data','rb'))\n","words=data['words']\n","classes=data['classes']\n","trainx=data['trainx']\n","trainy=data['trainy']"],"metadata":{"id":"p_0myCmit6aM","executionInfo":{"status":"ok","timestamp":1677387802595,"user_tz":-330,"elapsed":11,"user":{"displayName":"Satish Sahu","userId":"00112273144191307121"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["with open('intents.json') as json_data:\n","  intents=json.load(json_data)"],"metadata":{"id":"kTkkPcJdt6cv","executionInfo":{"status":"ok","timestamp":1677387802595,"user_tz":-330,"elapsed":11,"user":{"displayName":"Satish Sahu","userId":"00112273144191307121"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# loading the saved model\n","model.load('./model.tflearn')"],"metadata":{"id":"-l7dNpWot6fe","executionInfo":{"status":"ok","timestamp":1677387802595,"user_tz":-330,"elapsed":11,"user":{"displayName":"Satish Sahu","userId":"00112273144191307121"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# we will take inputs from user\n","# preprocess it into bag of words\n","def clean_sentences(sentence):\n","  # tokening the pattern\n","  sentence_words=nltk.word_tokenize(sentence)\n","  # stemming each word\n","  sentence_words=[stemmer.stem(word.lower()) for word in sentence_words]\n","  return sentence_words\n","\n","\n","# returning the bag of words array as 0/1 for each word in bag\n","def bow(sentence,words,show_details=False):\n","  # tokeninz the pattern\n","  sentence_words=clean_sentences(sentence)\n","  # generating bag of words\n","  bag=[0]*len(words)\n","  for s in sentence_words:\n","    for i,w in enumerate(words):\n","      if w==s:\n","        bag[i]=1\n","        if show_details:\n","          print(\"found in abd: %s\",w)\n","  return (np.array(bag))\n"],"metadata":{"id":"1l1Nys4Vt6ip","executionInfo":{"status":"ok","timestamp":1677387802595,"user_tz":-330,"elapsed":10,"user":{"displayName":"Satish Sahu","userId":"00112273144191307121"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["**Response** **Processor**"],"metadata":{"id":"yG2rrm_zJBwx"}},{"cell_type":"code","source":["# # WITHOUT CONTEXT\n","\n","# Error_threshold=0.30\n","# def classify(sentence):\n","#   # generate probabilities from model\n","#   results=model.predict([bow(sentence,words)])[0]\n","#   # filter out rpedictions below a threshold\n","#   results=[[i,r] for i,r in enumerate(results) if r>Error_threshold]\n","#   # sorting the strength of probabilties\n","#   results.sort(key=lambda x:x[1],reverse=True)\n","#   returnlist=[]\n","#   for r in results:\n","#     returnlist.append((classes[r[0]],r[1]))\n","#   # return tuple of intent and probability \n","#   return returnlist\n","\n","\n","# # here userID is the context about which conversations\n","# def response(sentence, userID='123',show_details=False):\n","#   results=classify(sentence)\n","#   # if we hahve a classification then find the best mathcng intent tag\n","#   if results:\n","#     # loop as long as there are matches to process\n","#     while results:\n","#       for i in intents['intents']:\n","#         # find  a tag matching the first result\n","#         if i['tag']==results[0][0]:\n","#           # a random respose from the intent\n","#           return print(random.choice(i['responses']))\n","#       results.pop(0)"],"metadata":{"id":"sBNGIb1OJApZ","executionInfo":{"status":"ok","timestamp":1677387802595,"user_tz":-330,"elapsed":10,"user":{"displayName":"Satish Sahu","userId":"00112273144191307121"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# # lets ask questions from chatbot\n","# classify('Bye')\n","# response('Bye')"],"metadata":{"id":"OZuRFvOVJAsh","executionInfo":{"status":"ok","timestamp":1677387802596,"user_tz":-330,"elapsed":11,"user":{"displayName":"Satish Sahu","userId":"00112273144191307121"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# WITH CONTEXT\n","\n","context={}\n","Error_threshold=0.30\n","def classify(sentence):\n","  # generate probabilities from model\n","  results=model.predict([bow(sentence,words)])[0]\n","  # filter out rpedictions below a threshold\n","  results=[[i,r] for i,r in enumerate(results) if r>Error_threshold]\n","  # sorting the strength of probabilties\n","  results.sort(key=lambda x:x[1],reverse=True)\n","  returnlist=[]\n","  for r in results:\n","    returnlist.append((classes[r[0]],r[1]))\n","  # return tuple of intent and probability \n","  return returnlist\n","\n","\n","# here userID is the context about which conversations\n","def response(sentence, userID='123',show_details=False):\n","  results=classify(sentence)\n","  # if we hahve a classification then find the best mathcng intent tag\n","  if results:\n","    # loop as long as there are matches to process\n","    while results:\n","      for i in intents['intents']:\n","        # find  a tag matching the first result\n","        if i['tag']==results[0][0]:\n","          #set context for this intent if necessary\n","          if 'context_set' in i:\n","            if show_details:print('context:',i['context_set'])\n","            context[userID]=i['context_set']\n","          \n","          # check if this intent is contextual and applies to users conversation\n","          if not 'context_filter' in i or \\\n","          (userID in context and 'context_filter' in i and i['context_filter']==context):\n","            if show_details:print('tag:',i['tag'])\n","            # a random respose from the intent\n","            return print(random.choice(i['responses']))\n","      results.pop(0)\n"],"metadata":{"id":"hRl8zJ46JAwB","executionInfo":{"status":"ok","timestamp":1677387802596,"user_tz":-330,"elapsed":11,"user":{"displayName":"Satish Sahu","userId":"00112273144191307121"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["response(\"bye\")"],"metadata":{"id":"P0Sd8KAIJAy8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677387802596,"user_tz":-330,"elapsed":11,"user":{"displayName":"Satish Sahu","userId":"00112273144191307121"}},"outputId":"19ead8f6-865b-43e6-ec40-d3b8ce4c25b6"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Bye! Come back again soon.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"rgJmu8opJA1Z","executionInfo":{"status":"ok","timestamp":1677387802596,"user_tz":-330,"elapsed":9,"user":{"displayName":"Satish Sahu","userId":"00112273144191307121"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wv8jCYfKJA37","executionInfo":{"status":"ok","timestamp":1677387802597,"user_tz":-330,"elapsed":10,"user":{"displayName":"Satish Sahu","userId":"00112273144191307121"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7-fw9zN-JA6Y","executionInfo":{"status":"ok","timestamp":1677387802597,"user_tz":-330,"elapsed":10,"user":{"displayName":"Satish Sahu","userId":"00112273144191307121"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"IEMXVH8Ut6nu","executionInfo":{"status":"ok","timestamp":1677387802597,"user_tz":-330,"elapsed":9,"user":{"displayName":"Satish Sahu","userId":"00112273144191307121"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2gj2B9Hjt6qW","executionInfo":{"status":"ok","timestamp":1677387802597,"user_tz":-330,"elapsed":9,"user":{"displayName":"Satish Sahu","userId":"00112273144191307121"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"T6NFeibqt6te","executionInfo":{"status":"ok","timestamp":1677387802597,"user_tz":-330,"elapsed":9,"user":{"displayName":"Satish Sahu","userId":"00112273144191307121"}}},"execution_count":22,"outputs":[]}]}